from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
import os
import streamlit as st
from langchain_openai import ChatOpenAI
import tempfile
from dotenv import load_dotenv, find_dotenv  # âœ… ì¶”ê°€

# âœ… .env ë¡œë“œ (ì•± ì‹œì‘ ì‹œ 1íšŒ)
load_dotenv(find_dotenv(), override=False)

embedding_model = HuggingFaceEmbeddings(model_name="distiluse-base-multilingual-cased-v1")

def vectorize(pdf):
    # BytesIOë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf.getvalue())
        file_path = tmp_file.name
    
    try:
        loader = PyPDFLoader(file_path)
        documents = loader.load()

        text_splitter = CharacterTextSplitter(
            separator='.',
            chunk_size=500,
            chunk_overlap=100,
            length_function=len
        )

        texts = text_splitter.split_documents(documents)

        db = FAISS.from_documents(texts, embedding_model)
        retriever = db.as_retriever(search_type='similarity', search_kwargs={"k": 3})
        return retriever
    finally:
        # ì„ì‹œ íŒŒì¼ ë°˜ë“œì‹œ ì‚­ì œ
        try:
            os.unlink(file_path)
        except OSError:
            pass

@st.cache_resource(show_spinner=False)
def load_openai_llm(model_name: str = "gpt-4o-mini", temperature: float = 0.0):
    # âœ… .envì—ì„œ ì˜¬ë¼ì˜¨ í™˜ê²½ë³€ìˆ˜ë§Œ ì‚¬ìš©
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError(
            "OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            ".envì— OPENAI_API_KEY=... í˜•íƒœë¡œ ë„£ê³  ì‹¤í–‰í•˜ì„¸ìš”."
        )
    return ChatOpenAI(model=model_name, temperature=temperature, api_key=api_key)

def search(query: str, retriever):
    docs = retriever.invoke(query)
    return docs or []

def build_prompt(query: str, docs):
    lines = []
    lines.append("ì•„ë˜ 'ìë£Œ'ë§Œ ê·¼ê±°ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”.")
    lines.append("- ìë£Œ ë°– ì •ë³´ë¥¼ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.")
    lines.append("- ë‹µí•  ìˆ˜ ì—†ìœ¼ë©´ 'ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'ë¼ê³  ë§í•˜ì„¸ìš”.\n")
    lines.append(f"ì§ˆë¬¸:\n{query}\n")
    lines.append("ìë£Œ:")
    for i, d in enumerate(docs, 1):
        lines.append(f"[ë¬¸ì„œ{i}]\n{d.page_content}\n")
    lines.append("ë‹µë³€:")
    return "\n".join(lines)

def generate_with_llm(llm: ChatOpenAI, prompt: str) -> str:
    resp = llm.invoke(prompt)
    return resp.content.strip()

def main():
    st.set_page_config(page_title="ğŸ¤– CHAT pdf", page_icon="ğŸ¤–", layout="centered")
    st.title("ğŸ¤– CHAT pdf")
    uploaded_file = st.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "llm" not in st.session_state:
        st.session_state.llm = load_openai_llm("gpt-4o-mini", temperature=0.0)

    if uploaded_file is not None:
        if "retriever" not in st.session_state:
            with st.spinner("PDF ì²˜ë¦¬ ì¤‘..."):
                st.session_state.retriever = vectorize(uploaded_file)

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_input = st.chat_input("pdfì—ê²Œ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”.")
    if user_input and "retriever" in st.session_state:
        st.chat_message("user").markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        query = user_input.strip()
        docs = search(query, st.session_state.retriever)
        if not docs:
            answer = "ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        else:
            prompt = build_prompt(query, docs)
            answer = generate_with_llm(st.session_state.llm, prompt)
        
        with st.chat_message("assistant"):
            st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})

if __name__ == "__main__":
    main()
